### 250715
# Data synchronication - Longhorn
### 여러 node 에 data sync 를 유지하고자 할 때 사용한다.
### 복제본이 있기 때문에 특정 node 에서 에러가 나더라도 복구할 수 있다.
### PVC를 자동 복제 + 복구를 제공하는 블록 스토리지 시스템이다.
### Longhorn Engine(storage controller)와 복제본은 쿠버네티스를 사용하여 자체적으로 오케스트레이션한다.
- /data 아래에 Longhorn 전용 디렉토리 예: /data/longhorn
- 모든 노드에 동일하게 존재하면 OK (용량은 달라도 됨)
- Longhorn 설치 후 StorageClass 생성됨 (longhorn)
- PVC에서 StorageClass를 longhorn으로 지정만 하면 끝
### <br/>

### Longhorn의 복제 방식 개요
- Longhorn은 볼륨당 N개의 복제본(replica) 을 다른 노드에 분산해서 저장함 (기본값: 3개)
- 한 Pod가 PVC를 통해 해당 볼륨을 하나의 노드에 마운트 하면,
- 그 노드에 있는 엔진(engine) 이 I/O를 처리하고,
- 엔진은 동시에 다른 노드에 있는 replica들에게 블록 단위로 write를 전파함
#### ✅ Pod가 쓰는 데이터는 실시간으로 복제 노드에 반영됨
### <br/>

### 예시 시나리오
- /data/sync/important.db에 write 발생
- Longhorn 엔진이 해당 블록을 로컬 replica + 2개 remote replica에 전송
- 쓰기 성공 후만 write 성공으로 처리됨 → write-through 방식
#### 📌 Crash나 장애가 발생해도 replica가 있으므로 복구 가능
### <br/>

### ⚠️ Longhorn의 중요한 특성과 제한사항
### longhorn은 데이터를 동기화하는 거라서 각 node에서 같은 PV 를 사용할 수는 없다.
### 같은 경로와 같은 데이터지만 각 node 에 있는 데이터를 마운트하여 사용하는 것이다.
### 이렇게 말고 각 node에서 동일한 걸 사용하게 하려면 네트워크 스토리지를 이용해야 한다.
| 항목                                                               | 
| ---------------------------------------------------------------- | 
| Pod는 항상 하나의 노드에서만 해당 PVC를 마운트함 (ReadWriteOnce)                   | 
| 다른 노드에서 동시에 해당 볼륨을 마운트할 수 없음                                     |
| 대신 해당 노드가 장애 나면, 다른 replica를 기반으로 다른 노드에서 자동 복구 가능               | 
| 모든 replica가 동시에 마운트되는 게 아니라, **하나는 active, 나머지는 passive mirror** | 
### <br/>

### 사용성
### 운영 환경에서 실제로 많이 쓰이고 있고, 특히 중소 규모의 Kubernetes 클러스터나 프라이빗 클러스터에 매우 적합하다.
### 다만, 대규모 환경이나 고성능이 절대적인 환경에서는 고려할 점도 있다. 
#### 1. Kubernetes 네이티브 블록 스토리지
- PVC 요청 → 자동 볼륨 생성 → 복제 & 마운트까지 완전 자동화
- CSI (Container Storage Interface) 지원, StorageClass 사용 가능 → DevOps 친화적
#### 2. 자동 복제 & 장애 복구
- 디폴트로 3개 노드에 데이터 복제
- 노드 장애 시 자동 failover (복구도 자동)
#### 3. 설치 쉬움 + Web UI 제공
- kubectl apply 한 줄로 설치
- 웹 UI에서 디스크 상태, 볼륨 상태 실시간 모니터링 가능
#### 4. 로컬 디스크 기반
- NFS, Ceph 등 외부 스토리지 필요 없음
- 자사 물리 서버만 있어도 고가용 스토리지 구성 가능
### <br/>

### ⚠️ 운영에서 고려할 점 (한계도 있음)
| 항목              | 내용                                             |
| --------------- | ---------------------------------------------- |
| 고성능 워크로드        | **네트워크 복제**로 인해 로컬 디스크만큼의 raw 성능은 안 나올 수 있음    |
| RWX 지원 제한       | 기본은 **ReadWriteOnce** → 여러 Pod가 동시에 공유하는 건 어려움 |
| 대규모 볼륨 수        | 수천 개 이상의 PVC나 replica가 많으면 관리 오버헤드 증가 가능       |
| 스토리지 전용 네트워크 권장 | 일반 서비스 트래픽과 분리하면 안정성과 성능 ↑                     |
