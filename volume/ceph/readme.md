### 250716
# Ceph
### Ceph는 오늘날 많이 사용되는 오픈소스 분산 스토리지 시스템으로, 클라우드나 Kubernetes 환경에서도 널리 활용된다.
### 모든 데이터를 자동으로 분산하고, 복제하고, 장애에 강하게 저장한다.
- 여러 서버의 디스크를 묶어 **하나의 가상 스토리지 풀(pool)** 처럼 만듭니다.
- 자체적으로 데이터를 **복제(replication)** 하여 **고가용성(HA)** 을 보장합니다.
- 스케일 아웃 구조: 서버와 디스크를 추가하면 용량과 성능이 확장됩니다.
### <br/>

### Ceph가 제공하는 3가지 스토리지 서비스
#### Kubernetes에서 Ceph를 사용할 때 가장 많이 쓰는 스토리지 서비스는 CephFS이다.
| 서비스                           | 설명              | 사용 예                  |
| ----------------------------- | --------------- | --------------------- |
| **RBD (RADOS Block Device)**  | 블록 스토리지         | VM 디스크, DB            |
| **CephFS (Ceph File System)** | POSIX 호환 파일 시스템 | 공유 볼륨, Kubernetes     |
| **RGW (RADOS Gateway)**       | S3 호환 오브젝트 스토리지 | 백업, 데이터 저장소, MinIO 대체 |

### <br/>

### Kubernetes 입장에서 왜 CephFS가 좋을까?
| 이유             | 설명                                             |
| -------------- | ---------------------------------------------- |
| ✅ 공유 가능 (RWX)  | 여러 노드의 여러 Pod가 동시에 마운트하여 사용 가능                 |
| ✅ CSI 지원       | `rook-ceph`로 CSI 자동 설치 및 PVC 연동                |
| ✅ POSIX 파일 시스템 | 기존 파일 기반 프로그램과 잘 호환됨 (`ls`, `cat`, `open()` 등) |
| ✅ 자동 확장/복제     | Ceph 클러스터 기능 그대로 이용 가능                         |

### <br/>

### Ceph의 주요 구성 요소
| 구성 요소                           | 설명                               |
| ------------------------------- | -------------------------------- |
| **MON (Monitor)**               | 클러스터 상태, 맵 관리, quorum 유지         |
| **OSD (Object Storage Daemon)** | 실제 데이터 저장, 복제, 복구                |
| **MGR (Manager)**               | 모니터링, 대시보드, 통계                   |
| **MDS (Metadata Server)**       | CephFS 전용. 디렉토리/파일 메타데이터 관리      |
| **Client**                      | Ceph를 사용하는 앱 또는 사용자 (예: 마운트한 서버) |

### <br/>

### 작동 원리 요약
1. 사용자는 파일/블록/오브젝트를 저장 요청
2. Ceph Client는 `CRUSH 알고리즘`을 사용하여 OSD들 중 저장 위치를 결정
3. 데이터는 여러 OSD에 자동 분산 + 복제
4. MON이 클러스터의 상태를 항상 감시
5. 노드/디스크 장애가 발생해도 복제된 데이터로 자동 복구
### <br/>

### 장단점
| 장점                            | 단점                           |
| ----------------------------- | ---------------------------- |
| ✅ 확장성 좋음 (스케일아웃)              | ⚠️ 초기 구성 복잡                  |
| ✅ 고가용성/장애 복구 자동화              | ⚠️ 자원 소모 큼 (MON, OSD, MGR 등) |
| ✅ 다양한 인터페이스 지원 (RBD, FS, S3)  | ⚠️ 네트워크 품질과 연동 필수            |
| ✅ 오픈소스 + 기업 지원 (RedHat, SUSE) | ⚠️ 디스크 불균형이 병목될 수 있음         |

### <br/>

### ⚠️ 중요: 가장 작은 노드가 전체 한계를 정함
#### 복제본은 모든 노드에 균등하게 분산되기 때문에,
#### **가장 작은 노드(500GB)**가 전체 사용 가능 용량을 제약할 수도 있다.
#### 예:
- 500GB 노드가 이미 90% 찼다면, 복제를 위해 더 이상 새 데이터를 넣지 못할 수도 있음
- Ceph는 자동으로 "이 노드엔 안 됨"이라고 판단하고 데이터를 분산 못 하게 됨
### <br/>

